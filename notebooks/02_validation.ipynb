{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed Minutes Parsing Validation\n",
    "\n",
    "This notebook provides comprehensive validation of the Fed Minutes parsing results, identifying and analyzing problematic files and overall data quality.\n",
    "\n",
    "## Contents\n",
    "1. Data Loading and Setup\n",
    "2. Overall Quality Assessment\n",
    "3. Problem File Identification\n",
    "4. Detailed Problem Analysis\n",
    "5. Sample File Examination\n",
    "6. Quality Improvement Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML, Markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "# Plot styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parsed data\n",
    "data_path = Path('../data/processed/meetings_summary.csv')\n",
    "\n",
    "if data_path.exists():\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"üìä Loaded data from {data_path}\")\n",
    "else:\n",
    "    # Try alternative locations\n",
    "    alt_paths = [\n",
    "        Path('../fed_minutes_output/meetings_summary.csv'),\n",
    "        Path('../data/validation/test_results.csv')\n",
    "    ]\n",
    "    \n",
    "    for alt_path in alt_paths:\n",
    "        if alt_path.exists():\n",
    "            df = pd.read_csv(alt_path)\n",
    "            print(f\"üìä Loaded data from {alt_path}\")\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find meetings data. Please run the parser first.\")\n",
    "\n",
    "# Parse JSON columns\n",
    "json_columns = ['attendees', 'decisions', 'topics', 'main_topics', 'board_members']\n",
    "for col in json_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(\n",
    "            lambda x: json.loads(x) if isinstance(x, str) and x.startswith('[') else x\n",
    "        )\n",
    "\n",
    "# Convert dates\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "print(f\"\\nüîç Dataset loaded:\")\n",
    "print(f\"  - Shape: {df.shape}\")\n",
    "print(f\"  - Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"  - Columns: {', '.join(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive quality metrics\n",
    "quality_metrics = {\n",
    "    'Data Completeness': {\n",
    "        'Date Extraction Success': f\"{df['date'].notna().mean()*100:.1f}%\",\n",
    "        'Files with Attendees': f\"{(df['num_attendees'] > 0).mean()*100:.1f}%\",\n",
    "        'Files with Decisions': f\"{(df['num_decisions'] > 0).mean()*100:.1f}%\",\n",
    "        'Files with Topics': f\"{(df['num_topics'] > 0).mean()*100:.1f}%\" if 'num_topics' in df.columns else 'N/A'\n",
    "    },\n",
    "    'Content Quality': {\n",
    "        'Average Attendees': f\"{df['num_attendees'].mean():.1f}\",\n",
    "        'Average Decisions': f\"{df['num_decisions'].mean():.1f}\",\n",
    "        'Average Text Length': f\"{df['text_length'].mean():,.0f} chars\" if 'text_length' in df.columns else 'N/A',\n",
    "        'Files with Adequate Attendance (‚â•3)': f\"{(df['num_attendees'] >= 3).mean()*100:.1f}%\"\n",
    "    },\n",
    "    'Potential Issues': {\n",
    "        'Low Attendance (<3)': f\"{(df['num_attendees'] < 3).sum()} files ({(df['num_attendees'] < 3).mean()*100:.1f}%)\",\n",
    "        'No Decisions': f\"{(df['num_decisions'] == 0).sum()} files ({(df['num_decisions'] == 0).mean()*100:.1f}%)\",\n",
    "        'Short Documents (<1000 chars)': f\"{(df['text_length'] < 1000).sum()} files ({(df['text_length'] < 1000).mean()*100:.1f}%)\" if 'text_length' in df.columns else 'N/A',\n",
    "        'Missing Dates': f\"{df['date'].isna().sum()} files ({df['date'].isna().mean()*100:.1f}%)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "display(Markdown(\"## üìä **Quality Assessment Dashboard**\"))\n",
    "\n",
    "for category, metrics in quality_metrics.items():\n",
    "    display(Markdown(f\"### {category}\"))\n",
    "    \n",
    "    # Create DataFrame for better display\n",
    "    metrics_df = pd.DataFrame(list(metrics.items()), columns=['Metric', 'Value'])\n",
    "    display(metrics_df)\n",
    "    print()  # Add spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual quality overview\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Distribution of key metrics\n",
    "axes[0,0].hist([df['num_attendees'], df['num_decisions']], \n",
    "               bins=20, alpha=0.7, label=['Attendees', 'Decisions'], color=['blue', 'orange'])\n",
    "axes[0,0].set_title('Distribution of Attendees and Decisions')\n",
    "axes[0,0].set_xlabel('Count')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Quality flags by year\n",
    "yearly_quality = df.groupby('year').agg({\n",
    "    'num_attendees': lambda x: (x < 3).mean() * 100,\n",
    "    'num_decisions': lambda x: (x == 0).mean() * 100\n",
    "}).rename(columns={'num_attendees': 'Low Attendance %', 'num_decisions': 'No Decisions %'})\n",
    "\n",
    "yearly_quality.plot(kind='bar', ax=axes[0,1], color=['red', 'darkred'], alpha=0.7)\n",
    "axes[0,1].set_title('Quality Issues by Year')\n",
    "axes[0,1].set_xlabel('Year')\n",
    "axes[0,1].set_ylabel('Percentage of Files')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot: Attendees vs Decisions\n",
    "colors = ['red' if (att < 3 or dec == 0) else 'blue' \n",
    "          for att, dec in zip(df['num_attendees'], df['num_decisions'])]\n",
    "\n",
    "axes[1,0].scatter(df['num_attendees'], df['num_decisions'], c=colors, alpha=0.6)\n",
    "axes[1,0].set_title('Attendees vs Decisions (Red = Problem Files)')\n",
    "axes[1,0].set_xlabel('Number of Attendees')\n",
    "axes[1,0].set_ylabel('Number of Decisions')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add problem threshold lines\n",
    "axes[1,0].axvline(x=3, color='red', linestyle='--', alpha=0.7, label='Min Attendees')\n",
    "axes[1,0].axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Min Decisions')\n",
    "axes[1,0].legend()\n",
    "\n",
    "# 4. Text length distribution (if available)\n",
    "if 'text_length' in df.columns:\n",
    "    axes[1,1].hist(df['text_length'], bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "    axes[1,1].axvline(1000, color='red', linestyle='--', label='Min Length Threshold')\n",
    "    axes[1,1].set_title('Document Length Distribution')\n",
    "    axes[1,1].set_xlabel('Text Length (characters)')\n",
    "    axes[1,1].set_ylabel('Frequency')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1,1].text(0.5, 0.5, 'Text length data\\nnot available', \n",
    "                   ha='center', va='center', transform=axes[1,1].transAxes,\n",
    "                   fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray'))\n",
    "    axes[1,1].set_title('Document Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Problem File Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criteria for problem files\n",
    "low_attendance = df['num_attendees'] < 3\n",
    "no_decisions = df['num_decisions'] == 0\n",
    "missing_date = df['date'].isna()\n",
    "short_text = df['text_length'] < 1000 if 'text_length' in df.columns else pd.Series([False] * len(df))\n",
    "\n",
    "# Identify problem files\n",
    "problem_files = df[\n",
    "    low_attendance | no_decisions | missing_date | short_text\n",
    "].copy()\n",
    "\n",
    "# Categorize issues\n",
    "problem_files['issues'] = problem_files.apply(lambda row: \n",
    "    '; '.join([\n",
    "        'Low Attendance' if row['num_attendees'] < 3 else '',\n",
    "        'No Decisions' if row['num_decisions'] == 0 else '',\n",
    "        'Missing Date' if pd.isna(row['date']) else '',\n",
    "        'Short Text' if 'text_length' in row and row['text_length'] < 1000 else ''\n",
    "    ]).strip('; ').replace(';; ', '; ').replace(';;', ''), axis=1)\n",
    "\n",
    "# Remove empty issues\n",
    "problem_files['issues'] = problem_files['issues'].str.replace('^; |; $', '', regex=True)\n",
    "problem_files = problem_files[problem_files['issues'] != '']\n",
    "\n",
    "display(Markdown(f\"## ‚ö†Ô∏è **Found {len(problem_files)} Problem Files** (out of {len(df)} total = {len(problem_files)/len(df)*100:.1f}%)\"))\n",
    "\n",
    "# Issue breakdown\n",
    "issue_breakdown = {\n",
    "    'Low Attendance (<3 people)': low_attendance.sum(),\n",
    "    'No Decisions Found': no_decisions.sum(),\n",
    "    'Missing Date': missing_date.sum(),\n",
    "    'Short Text (<1000 chars)': short_text.sum() if 'text_length' in df.columns else 0\n",
    "}\n",
    "\n",
    "# Remove zero counts\n",
    "issue_breakdown = {k: v for k, v in issue_breakdown.items() if v > 0}\n",
    "\n",
    "breakdown_df = pd.DataFrame([\n",
    "    {'Issue Type': issue, 'Count': count, 'Percentage': f\"{count/len(df)*100:.1f}%\"}\n",
    "    for issue, count in issue_breakdown.items()\n",
    "])\n",
    "\n",
    "display(breakdown_df)\n",
    "\n",
    "# Overlap analysis\n",
    "overlap_analysis = {\n",
    "    'Low Attendance Only': (low_attendance & ~no_decisions).sum(),\n",
    "    'No Decisions Only': (~low_attendance & no_decisions).sum(),\n",
    "    'Both Issues': (low_attendance & no_decisions).sum()\n",
    "}\n",
    "\n",
    "if any(overlap_analysis.values()):\n",
    "    display(Markdown(\"### Issue Overlap Analysis\"))\n",
    "    overlap_df = pd.DataFrame([\n",
    "        {'Category': cat, 'Count': count, 'Percentage': f\"{count/len(df)*100:.1f}%\"}\n",
    "        for cat, count in overlap_analysis.items() if count > 0\n",
    "    ])\n",
    "    display(overlap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize problem file distribution\n",
    "if len(problem_files) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Problem files by year\n",
    "    if 'year' in problem_files.columns:\n",
    "        problem_by_year = problem_files.groupby('year').size()\n",
    "        total_by_year = df.groupby('year').size()\n",
    "        problem_rate = (problem_by_year / total_by_year * 100).fillna(0)\n",
    "        \n",
    "        axes[0,0].bar(problem_by_year.index, problem_by_year.values, color='coral', alpha=0.7)\n",
    "        axes[0,0].set_title('Problem Files by Year')\n",
    "        axes[0,0].set_xlabel('Year')\n",
    "        axes[0,0].set_ylabel('Number of Problem Files')\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Problem rate by year\n",
    "    if 'year' in problem_files.columns:\n",
    "        axes[0,1].plot(problem_rate.index, problem_rate.values, marker='o', linewidth=2, color='red')\n",
    "        axes[0,1].set_title('Problem Rate by Year')\n",
    "        axes[0,1].set_xlabel('Year')\n",
    "        axes[0,1].set_ylabel('Problem Rate (%)')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Issue types distribution\n",
    "    if issue_breakdown:\n",
    "        issues, counts = zip(*issue_breakdown.items())\n",
    "        axes[1,0].pie(counts, labels=issues, autopct='%1.1f%%', startangle=90)\n",
    "        axes[1,0].set_title('Distribution of Issue Types')\n",
    "    \n",
    "    # 4. Problem vs Normal files comparison\n",
    "    normal_files = df[~df.index.isin(problem_files.index)]\n",
    "    \n",
    "    comparison_data = {\n",
    "        'Avg Attendees': [normal_files['num_attendees'].mean(), problem_files['num_attendees'].mean()],\n",
    "        'Avg Decisions': [normal_files['num_decisions'].mean(), problem_files['num_decisions'].mean()]\n",
    "    }\n",
    "    \n",
    "    x = np.arange(len(comparison_data))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,1].bar(x - width/2, [comparison_data['Avg Attendees'][0], comparison_data['Avg Decisions'][0]], \n",
    "                  width, label='Normal Files', color='green', alpha=0.7)\n",
    "    axes[1,1].bar(x + width/2, [comparison_data['Avg Attendees'][1], comparison_data['Avg Decisions'][1]], \n",
    "                  width, label='Problem Files', color='red', alpha=0.7)\n",
    "    \n",
    "    axes[1,1].set_title('Normal vs Problem Files Comparison')\n",
    "    axes[1,1].set_xticks(x)\n",
    "    axes[1,1].set_xticklabels(['Avg Attendees', 'Avg Decisions'])\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    display(Markdown(\"### üéâ **No significant problems found in the dataset!**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Problem Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(problem_files) > 0:\n",
    "    # Show detailed problem file list\n",
    "    display(Markdown(\"### üìã **Problem Files Details**\"))\n",
    "    \n",
    "    # Prepare display columns\n",
    "    display_cols = ['filename', 'date', 'num_attendees', 'num_decisions', 'issues']\n",
    "    if 'text_length' in problem_files.columns:\n",
    "        display_cols.insert(-1, 'text_length')\n",
    "    \n",
    "    problem_display = problem_files[display_cols].copy()\n",
    "    if 'date' in problem_display.columns:\n",
    "        problem_display['date'] = problem_display['date'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Sort by date if available, otherwise by filename\n",
    "    if 'date' in problem_display.columns:\n",
    "        problem_display = problem_display.sort_values('date')\n",
    "    else:\n",
    "        problem_display = problem_display.sort_values('filename')\n",
    "    \n",
    "    # Show first 20 rows\n",
    "    display(Markdown(f\"**Showing first 20 of {len(problem_files)} problem files:**\"))\n",
    "    display(problem_display.head(20))\n",
    "    \n",
    "    # Save complete list\n",
    "    output_path = Path('../data/validation/problematic_files_detailed.csv')\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    problem_display.to_csv(output_path, index=False)\n",
    "    print(f\"\\nüíæ Complete problem files list saved to: {output_path}\")\n",
    "\n",
    "else:\n",
    "    display(Markdown(\"### ‚úÖ **No problem files to analyze!**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample File Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_file_content(filename, df):\n",
    "    \"\"\"Examine a specific file in detail\"\"\"\n",
    "    try:\n",
    "        row = df[df['filename'] == filename].iloc[0]\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìÑ FILE: {filename}\")\n",
    "        print('='*60)\n",
    "        print(f\"üìÖ Date: {row['date'] if pd.notna(row['date']) else 'MISSING'}\")\n",
    "        print(f\"üë• Attendees: {row['num_attendees']}\")\n",
    "        print(f\"üìã Decisions: {row['num_decisions']}\")\n",
    "        if 'text_length' in row:\n",
    "            print(f\"üìù Text length: {row['text_length']:,} characters\")\n",
    "        \n",
    "        # Show attendees if available\n",
    "        if 'attendees' in row and isinstance(row['attendees'], list) and len(row['attendees']) > 0:\n",
    "            print(f\"\\nüë• Attendees list:\")\n",
    "            for i, attendee in enumerate(row['attendees'][:5], 1):\n",
    "                if isinstance(attendee, dict):\n",
    "                    name = attendee.get('name', 'Unknown')\n",
    "                    title = attendee.get('title', '')\n",
    "                    print(f\"   {i}. {name}\" + (f\" - {title}\" if title else \"\"))\n",
    "                else:\n",
    "                    print(f\"   {i}. {attendee}\")\n",
    "            if len(row['attendees']) > 5:\n",
    "                print(f\"   ... and {len(row['attendees']) - 5} more\")\n",
    "        \n",
    "        # Show decisions if available\n",
    "        if 'decisions' in row and isinstance(row['decisions'], list) and len(row['decisions']) > 0:\n",
    "            print(f\"\\nüìã Decisions:\")\n",
    "            for i, decision in enumerate(row['decisions'][:3], 1):\n",
    "                if isinstance(decision, dict):\n",
    "                    action = decision.get('action', 'Unknown')\n",
    "                    subject = decision.get('subject', 'No subject')\n",
    "                    print(f\"   {i}. {action.upper()}: {subject[:80]}...\")\n",
    "                else:\n",
    "                    print(f\"   {i}. {decision}\")\n",
    "            if len(row['decisions']) > 3:\n",
    "                print(f\"   ... and {len(row['decisions']) - 3} more\")\n",
    "        \n",
    "        # Try to show original text preview\n",
    "        txt_paths = [\n",
    "            Path(f\"../data/raw/TXTs/{filename.replace('.pdf', '.txt')}\"),\n",
    "            Path(f\"../TXTs/{filename.replace('.pdf', '.txt')}\"),\n",
    "            Path(f\"TXTs/{filename.replace('.pdf', '.txt')}\")\n",
    "        ]\n",
    "        \n",
    "        for txt_path in txt_paths:\n",
    "            if txt_path.exists():\n",
    "                with open(txt_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                print(f\"\\nüìñ Text Preview (first 500 characters):\")\n",
    "                print(\"-\" * 50)\n",
    "                print(text[:500])\n",
    "                print(\"-\" * 50)\n",
    "                break\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Could not find text file for {filename}\")\n",
    "                \n",
    "    except IndexError:\n",
    "        print(f\"‚ùå File {filename} not found in dataset\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error examining file {filename}: {e}\")\n",
    "\n",
    "# Examine sample problem files\n",
    "if len(problem_files) > 0:\n",
    "    display(Markdown(\"## üîç **Sample Problem File Examination**\"))\n",
    "    \n",
    "    # Select diverse sample files\n",
    "    sample_files = []\n",
    "    \n",
    "    # Get files with different types of issues\n",
    "    if (problem_files['num_attendees'] < 3).any():\n",
    "        low_att_file = problem_files[problem_files['num_attendees'] < 3]['filename'].iloc[0]\n",
    "        sample_files.append(low_att_file)\n",
    "    \n",
    "    if (problem_files['num_decisions'] == 0).any():\n",
    "        no_dec_file = problem_files[problem_files['num_decisions'] == 0]['filename'].iloc[0]\n",
    "        if no_dec_file not in sample_files:\n",
    "            sample_files.append(no_dec_file)\n",
    "    \n",
    "    # Add one more random problem file if available\n",
    "    if len(problem_files) > len(sample_files):\n",
    "        remaining_files = problem_files[~problem_files['filename'].isin(sample_files)]\n",
    "        if len(remaining_files) > 0:\n",
    "            sample_files.append(remaining_files['filename'].iloc[0])\n",
    "    \n",
    "    # Limit to 3 files for readability\n",
    "    sample_files = sample_files[:3]\n",
    "    \n",
    "    print(f\"üìã Examining {len(sample_files)} sample problem files:\")\n",
    "    \n",
    "    for filename in sample_files:\n",
    "        examine_file_content(filename, df)\n",
    "\n",
    "else:\n",
    "    display(Markdown(\"## ‚úÖ **No problem files to examine!**\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quality Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive quality score\n",
    "def calculate_quality_score(df):\n",
    "    \"\"\"Calculate overall parsing quality score\"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    weights = {}\n",
    "    \n",
    "    # Date extraction score\n",
    "    scores['date_extraction'] = df['date'].notna().mean()\n",
    "    weights['date_extraction'] = 0.25\n",
    "    \n",
    "    # Attendee quality score (normalized, capped at 1.0)\n",
    "    scores['attendee_quality'] = min(1.0, df['num_attendees'].mean() / 10)\n",
    "    weights['attendee_quality'] = 0.20\n",
    "    \n",
    "    # Decision quality score (normalized, capped at 1.0)\n",
    "    scores['decision_quality'] = min(1.0, df['num_decisions'].mean() / 5)\n",
    "    weights['decision_quality'] = 0.25\n",
    "    \n",
    "    # Content completeness (files with adequate attendance)\n",
    "    scores['content_completeness'] = (df['num_attendees'] >= 3).mean()\n",
    "    weights['content_completeness'] = 0.15\n",
    "    \n",
    "    # Text quality (if available)\n",
    "    if 'text_length' in df.columns:\n",
    "        scores['text_quality'] = (df['text_length'] >= 1000).mean()\n",
    "        weights['text_quality'] = 0.15\n",
    "    else:\n",
    "        # Redistribute weight to other categories\n",
    "        weights['date_extraction'] = 0.30\n",
    "        weights['attendee_quality'] = 0.25\n",
    "        weights['decision_quality'] = 0.30\n",
    "        weights['content_completeness'] = 0.15\n",
    "    \n",
    "    # Calculate weighted average\n",
    "    overall_score = sum(scores[k] * weights[k] for k in scores if k in weights) * 100\n",
    "    \n",
    "    return scores, weights, overall_score\n",
    "\n",
    "scores, weights, overall_score = calculate_quality_score(df)\n",
    "\n",
    "display(Markdown(\"## üéØ **Overall Quality Assessment**\"))\n",
    "\n",
    "# Display component scores\n",
    "component_df = pd.DataFrame([\n",
    "    {\n",
    "        'Component': component.replace('_', ' ').title(),\n",
    "        'Score': f\"{score*100:.1f}%\",\n",
    "        'Weight': f\"{weights.get(component, 0)*100:.0f}%\"\n",
    "    }\n",
    "    for component, score in scores.items()\n",
    "])\n",
    "\n",
    "display(component_df)\n",
    "\n",
    "print(f\"\\nüèÜ **OVERALL QUALITY SCORE: {overall_score:.1f}%**\")\n",
    "\n",
    "# Quality assessment\n",
    "if overall_score >= 90:\n",
    "    assessment = \"üéâ **EXCELLENT** - Ready for analysis and production use!\"\n",
    "    color = \"green\"\n",
    "elif overall_score >= 80:\n",
    "    assessment = \"‚úÖ **GOOD** - Minor improvements could enhance quality\"\n",
    "    color = \"orange\"\n",
    "elif overall_score >= 70:\n",
    "    assessment = \"‚ö†Ô∏è **ACCEPTABLE** - Several issues should be addressed\"\n",
    "    color = \"orange\"\n",
    "else:\n",
    "    assessment = \"‚ùå **NEEDS IMPROVEMENT** - Significant parsing issues detected\"\n",
    "    color = \"red\"\n",
    "\n",
    "display(Markdown(f\"### {assessment}\"))\n",
    "\n",
    "# Create quality score visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Component scores bar chart\n",
    "components = [comp.replace('_', ' ').title() for comp in scores.keys()]\n",
    "score_values = [score * 100 for score in scores.values()]\n",
    "\n",
    "bars = ax1.barh(components, score_values, color=['green' if s >= 90 else 'orange' if s >= 70 else 'red' for s in score_values])\n",
    "ax1.set_title('Quality Component Scores')\n",
    "ax1.set_xlabel('Score (%)')\n",
    "ax1.set_xlim(0, 100)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add score labels\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.1f}%', ha='left', va='center')\n",
    "\n",
    "# Overall score gauge\n",
    "ax2.pie([overall_score, 100-overall_score], \n",
    "        colors=[color, 'lightgray'], \n",
    "        startangle=90,\n",
    "        counterclock=False)\n",
    "ax2.add_patch(plt.Circle((0, 0), 0.6, color='white'))\n",
    "ax2.text(0, 0, f'{overall_score:.1f}%', ha='center', va='center', \n",
    "         fontsize=20, fontweight='bold')\n",
    "ax2.set_title('Overall Quality Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Recommendations and Action Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on findings\n",
    "recommendations = []\n",
    "\n",
    "if (df['num_attendees'] < 3).sum() > 0:\n",
    "    low_att_count = (df['num_attendees'] < 3).sum()\n",
    "    recommendations.append({\n",
    "        'Priority': 'High' if low_att_count > len(df) * 0.05 else 'Medium',\n",
    "        'Issue': f'{low_att_count} files with low attendance (<3 people)',\n",
    "        'Recommendation': 'Review attendee extraction patterns; check for OCR issues in PRESENT sections',\n",
    "        'Action': 'Improve attendee parsing algorithm'\n",
    "    })\n",
    "\n",
    "if (df['num_decisions'] == 0).sum() > 0:\n",
    "    no_dec_count = (df['num_decisions'] == 0).sum()\n",
    "    recommendations.append({\n",
    "        'Priority': 'High' if no_dec_count > len(df) * 0.05 else 'Medium',\n",
    "        'Issue': f'{no_dec_count} files with no decisions found',\n",
    "        'Recommendation': 'Expand decision action verb patterns; check for alternative decision formats',\n",
    "        'Action': 'Enhance decision extraction patterns'\n",
    "    })\n",
    "\n",
    "if df['date'].isna().sum() > 0:\n",
    "    missing_dates = df['date'].isna().sum()\n",
    "    recommendations.append({\n",
    "        'Priority': 'Medium',\n",
    "        'Issue': f'{missing_dates} files with missing dates',\n",
    "        'Recommendation': 'Add more date format patterns; check document headers and footers',\n",
    "        'Action': 'Improve date extraction patterns'\n",
    "    })\n",
    "\n",
    "if 'text_length' in df.columns and (df['text_length'] < 1000).sum() > 0:\n",
    "    short_docs = (df['text_length'] < 1000).sum()\n",
    "    recommendations.append({\n",
    "        'Priority': 'Low',\n",
    "        'Issue': f'{short_docs} files with very short text (<1000 chars)',\n",
    "        'Recommendation': 'Check PDF extraction quality; may indicate scan quality issues',\n",
    "        'Action': 'Review PDF-to-text conversion process'\n",
    "    })\n",
    "\n",
    "# Add positive notes if quality is high\n",
    "if overall_score >= 90:\n",
    "    recommendations.append({\n",
    "        'Priority': 'Info',\n",
    "        'Issue': 'High overall quality achieved',\n",
    "        'Recommendation': 'Dataset is ready for analysis. Consider expanding to full corpus.',\n",
    "        'Action': 'Proceed with full processing pipeline'\n",
    "    })\n",
    "\n",
    "display(Markdown(\"## üìù **Recommendations and Action Items**\"))\n",
    "\n",
    "if recommendations:\n",
    "    rec_df = pd.DataFrame(recommendations)\n",
    "    display(rec_df)\n",
    "else:\n",
    "    display(Markdown(\"### ‚úÖ **No specific recommendations - Quality is excellent!**\"))\n",
    "\n",
    "# Summary report\n",
    "display(Markdown(\"## üìä **Validation Summary Report**\"))\n",
    "\n",
    "summary_report = {\n",
    "    'validation_date': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_files': len(df),\n",
    "        'date_range': f\"{df['date'].min()} to {df['date'].max()}\",\n",
    "        'problem_files': len(problem_files) if len(problem_files) > 0 else 0,\n",
    "        'problem_rate': f\"{len(problem_files)/len(df)*100:.2f}%\" if len(problem_files) > 0 else \"0%\"\n",
    "    },\n",
    "    'quality_scores': {k: f\"{v*100:.1f}%\" for k, v in scores.items()},\n",
    "    'overall_score': f\"{overall_score:.1f}%\",\n",
    "    'assessment': assessment,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "print(f\"üìã VALIDATION SUMMARY:\")\n",
    "print(f\"  ‚Ä¢ Total Files: {summary_report['dataset_info']['total_files']}\")\n",
    "print(f\"  ‚Ä¢ Problem Files: {summary_report['dataset_info']['problem_files']} ({summary_report['dataset_info']['problem_rate']})\")\n",
    "print(f\"  ‚Ä¢ Overall Quality Score: {summary_report['overall_score']}\")\n",
    "print(f\"  ‚Ä¢ Assessment: {assessment}\")\n",
    "\n",
    "# Save validation report\n",
    "report_path = Path('../data/validation/validation_report.json')\n",
    "report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nüíæ Complete validation report saved to: {report_path}\")\n",
    "print(\"\\nüéâ Validation analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}