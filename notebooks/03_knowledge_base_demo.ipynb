{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed Minutes Knowledge Base Demonstration\n",
    "\n",
    "This notebook demonstrates the Phase 2 knowledge base for semantic search and analysis of Federal Reserve meeting minutes (1965-1973).\n",
    "\n",
    "## Overview\n",
    "The knowledge base transforms 1,100+ Fed meeting minutes into an intelligent search system using:\n",
    "- **Vector embeddings** for semantic understanding\n",
    "- **ChromaDB** for fast similarity search\n",
    "- **Rich metadata** preservation for context\n",
    "- **Temporal analysis** capabilities\n",
    "\n",
    "## Contents\n",
    "1. [Setup & Data Loading](#1-setup--data-loading)\n",
    "2. [Vector Embeddings](#2-vector-embeddings)\n",
    "3. [Database Creation](#3-database-creation)\n",
    "4. [Semantic Search Examples](#4-semantic-search-examples)\n",
    "5. [Advanced Analysis](#5-advanced-analysis)\n",
    "6. [Research Applications](#6-research-applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "plt.style.use('default')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import knowledge base modules\n",
    "from src.utils.config import load_config\n",
    "from src.phase2_knowledge_base import (\n",
    "    create_embeddings_pipeline,\n",
    "    create_vector_db,\n",
    "    create_search_interface,\n",
    "    DocumentChunk,\n",
    "    QueryBuilder\n",
    ")\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(\"‚úì Knowledge base modules loaded\")\n",
    "print(\"‚úì Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parsed meetings data\n",
    "processed_dir = Path(config['paths']['processed_dir'])\n",
    "meetings_file = processed_dir / 'meetings_full.json'\n",
    "\n",
    "if not meetings_file.exists():\n",
    "    print(\"‚ùå No parsed meetings found.\")\n",
    "    print(\"   Please run Phase 1 parsing first:\")\n",
    "    print(\"   python -m src.phase1_parsing.fed_parser\")\n",
    "    raise FileNotFoundError(f\"Missing: {meetings_file}\")\n",
    "\n",
    "# Load meetings data\n",
    "df_meetings = pd.read_json(meetings_file)\n",
    "print(f\"‚úì Loaded {len(df_meetings):,} meetings\")\n",
    "print(f\"  Date range: {df_meetings['date'].min()} to {df_meetings['date'].max()}\")\n",
    "\n",
    "# Verify raw_text column exists\n",
    "if 'raw_text' not in df_meetings.columns:\n",
    "    print(\"‚ùå Missing raw_text column\")\n",
    "    print(\"   Please run: python3 fix_json.py\")\n",
    "    raise ValueError(\"raw_text column required for embeddings\")\n",
    "\n",
    "avg_text_length = df_meetings['raw_text'].str.len().mean()\n",
    "print(f\"‚úì Raw text available (avg: {avg_text_length:,.0f} chars per meeting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data overview\n",
    "print(\"üìä Fed Minutes Dataset Overview\\n\")\n",
    "\n",
    "# Basic statistics\n",
    "stats = {\n",
    "    'Total Meetings': f\"{len(df_meetings):,}\",\n",
    "    'Date Range': f\"{df_meetings['date'].min().strftime('%Y-%m-%d')} to {df_meetings['date'].max().strftime('%Y-%m-%d')}\",\n",
    "    'Avg Attendees': f\"{df_meetings['num_attendees'].mean():.1f}\",\n",
    "    'Avg Decisions': f\"{df_meetings['num_decisions'].mean():.1f}\",\n",
    "    'Avg Topics': f\"{df_meetings['num_topics'].mean():.1f}\",\n",
    "    'Total Text': f\"{df_meetings['raw_text'].str.len().sum():,} characters\"\n",
    "}\n",
    "\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:15}: {value}\")\n",
    "\n",
    "# Meeting types distribution\n",
    "print(f\"\\nMeeting Types:\")\n",
    "meeting_types = df_meetings['meeting_type'].value_counts()\n",
    "for meeting_type, count in meeting_types.head().items():\n",
    "    print(f\"  {meeting_type:12}: {count:,} meetings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vector Embeddings\n",
    "\n",
    "Transform meeting text into vector embeddings for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing embeddings\n",
    "embeddings_dir = processed_dir / 'embeddings'\n",
    "embeddings_file = embeddings_dir / 'embeddings.npy'\n",
    "chunks_file = embeddings_dir / 'document_chunks.json'\n",
    "\n",
    "# Initialize chunks_data\n",
    "chunks_data = None\n",
    "\n",
    "if embeddings_file.exists() and chunks_file.exists():\n",
    "    print(\"üìÅ Loading existing embeddings...\")\n",
    "    \n",
    "    # Load existing embeddings and chunks\n",
    "    embeddings = np.load(embeddings_file)\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úì Loaded {len(chunks_data):,} chunks\")\n",
    "    print(f\"‚úì Embedding dimension: {embeddings.shape[1]}\")\n",
    "    print(f\"‚úì Model: {config['embedding']['model']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"üî® Building embeddings from meeting text...\")\n",
    "    print(\"   This process may take 5-10 minutes\")\n",
    "    \n",
    "    # Create embedding pipeline\n",
    "    pipeline = create_embeddings_pipeline(config)\n",
    "    \n",
    "    # Process meetings into chunks\n",
    "    print(\"   Step 1: Chunking meeting text...\")\n",
    "    chunks = pipeline.process_meetings_dataframe(df_meetings)\n",
    "    print(f\"   ‚úì Created {len(chunks):,} document chunks\")\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"   Step 2: Generating vector embeddings...\")\n",
    "    chunks, embeddings = pipeline.generate_embeddings_for_chunks(chunks)\n",
    "    print(f\"   ‚úì Generated embeddings: {embeddings.shape}\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"   Step 3: Saving to disk...\")\n",
    "    pipeline.save_processed_data(chunks, embeddings, str(embeddings_dir))\n",
    "    \n",
    "    # Load saved data for consistency\n",
    "    with open(chunks_file, 'r') as f:\n",
    "        chunks_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úì Embeddings saved to {embeddings_dir}\")\n",
    "    print(f\"‚úì Ready for semantic search with {len(chunks_data):,} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample chunk structure\n",
    "if chunks_data and len(chunks_data) > 0:\n",
    "    sample_chunk = chunks_data[0]\n",
    "    \n",
    "    print(\"üìù Sample Document Chunk Structure:\\n\")\n",
    "    print(f\"Chunk ID: {sample_chunk['chunk_id']}\")\n",
    "    print(f\"Meeting:  {sample_chunk['filename']} ({sample_chunk['date'][:10]})\")\n",
    "    print(f\"Type:     {sample_chunk['meeting_type']}\")\n",
    "    print(f\"Topics:   {', '.join(sample_chunk['topics'][:3]) if sample_chunk['topics'] else 'None'}\")\n",
    "    print(f\"\\nText Preview ({len(sample_chunk['chunk_text'])} chars):\")\n",
    "    print(\"‚îÄ\" * 50)\n",
    "    print(sample_chunk['chunk_text'][:300] + \"...\")\n",
    "    print(\"‚îÄ\" * 50)\n",
    "else:\n",
    "    print(\"‚ùå No chunk data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Database Creation\n",
    "\n",
    "Load chunks into ChromaDB vector database for fast semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector database\n",
    "if not chunks_data:\n",
    "    print(\"‚ùå Cannot create database without chunk data\")\n",
    "    raise RuntimeError(\"Chunks data not available\")\n",
    "\n",
    "print(\"üóÑÔ∏è  Initializing ChromaDB vector database...\")\n",
    "vector_db = create_vector_db(config, reset=False)\n",
    "\n",
    "# Check database status\n",
    "stats = vector_db.get_collection_stats()\n",
    "print(f\"\\nüìä Database Statistics:\")\n",
    "print(f\"  Total chunks: {stats['total_chunks']:,}\")\n",
    "print(f\"  Date range: {stats['date_range']['earliest']} to {stats['date_range']['latest']}\")\n",
    "print(f\"  Meeting types: {len(stats['meeting_types'])}\")\n",
    "print(f\"  Collection: {stats['collection_name']}\")\n",
    "print(f\"  Model: {stats['embedding_model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chunks into database if empty\n",
    "if stats['total_chunks'] == 0:\n",
    "    print(\"üì• Loading chunks into database...\")\n",
    "    print(\"   This may take 2-3 minutes\")\n",
    "    \n",
    "    # Convert to DocumentChunk objects\n",
    "    from src.phase2_knowledge_base.vector_embeddings import DocumentChunk\n",
    "    \n",
    "    chunks_objects = []\n",
    "    for chunk_data in chunks_data:\n",
    "        chunk = DocumentChunk(\n",
    "            chunk_id=chunk_data['chunk_id'],\n",
    "            meeting_id=chunk_data['meeting_id'],\n",
    "            filename=chunk_data['filename'],\n",
    "            date=datetime.fromisoformat(chunk_data['date']) if chunk_data['date'] else None,\n",
    "            chunk_text=chunk_data['chunk_text'],\n",
    "            chunk_index=chunk_data['chunk_index'],\n",
    "            total_chunks=chunk_data['total_chunks'],\n",
    "            meeting_type=chunk_data['meeting_type'],\n",
    "            attendees=chunk_data['attendees'],\n",
    "            topics=chunk_data['topics'],\n",
    "            decisions_summary=chunk_data['decisions_summary'],\n",
    "            page_references=chunk_data['page_references']\n",
    "        )\n",
    "        chunks_objects.append(chunk)\n",
    "    \n",
    "    # Add to database in batches\n",
    "    vector_db.add_document_chunks(chunks_objects, batch_size=100)\n",
    "    \n",
    "    # Verify loading\n",
    "    stats = vector_db.get_collection_stats()\n",
    "    print(f\"‚úì Database populated with {stats['total_chunks']:,} chunks\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚úì Database already contains data\")\n",
    "\n",
    "print(f\"\\nüöÄ Vector database ready for semantic search!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search Examples\n",
    "\n",
    "Demonstrate various search capabilities of the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize search interface\n",
    "if stats['total_chunks'] > 0:\n",
    "    search = create_search_interface(config)\n",
    "    print(\"üîç Semantic search interface initialized\")\n",
    "    print(f\"   Ready to search {stats['total_chunks']:,} document chunks\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize search - database is empty\")\n",
    "    raise RuntimeError(\"Database not populated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Monetary Policy Search\n",
    "print(\"üéØ Example 1: Monetary Policy Discussions\\n\")\n",
    "\n",
    "query = \"interest rates and monetary policy decisions\"\n",
    "results = search.search(query, max_results=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Found: {results['total_results']} results\\n\")\n",
    "\n",
    "for i, result in enumerate(results['results'], 1):\n",
    "    print(f\"üìÑ Result {i}:\")\n",
    "    print(f\"   Meeting: {result['filename']} ({result['date'][:10]})\")\n",
    "    print(f\"   Similarity: {result['similarity_score']:.4f}\")\n",
    "    print(f\"   Topics: {', '.join(result['topics'][:3]) if result['topics'] else 'N/A'}\")\n",
    "    print(f\"   Preview: {result['chunk_text'][:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: International Finance\n",
    "print(\"üåç Example 2: International Financial Coordination\\n\")\n",
    "\n",
    "query = \"international monetary cooperation foreign exchange intervention\"\n",
    "results = search.search(query, max_results=3)\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Found: {results['total_results']} results\\n\")\n",
    "\n",
    "for i, result in enumerate(results['results'], 1):\n",
    "    print(f\"üåê Result {i}: {result['filename']} ({result['date'][:10]})\")\n",
    "    print(f\"   Similarity: {result['similarity_score']:.4f}\")\n",
    "    print(f\"   Text: {result['chunk_text'][:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-Filtered Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Nixon Shock Period Analysis\n",
    "print(\"‚ö° Example 3: Inflation Concerns Around Nixon Shock\\n\")\n",
    "\n",
    "query = \"inflation price stability wage controls\"\n",
    "date_range = (\"1971-07-01\", \"1972-03-31\")\n",
    "\n",
    "results = search.search(\n",
    "    query=query,\n",
    "    date_range=date_range,\n",
    "    max_results=5\n",
    ")\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(f\"Period: {date_range[0]} to {date_range[1]} (Nixon Shock era)\")\n",
    "print(f\"Found: {results['total_results']} results\")\n",
    "\n",
    "if results['results']:\n",
    "    print(\"\\nüìÖ Timeline of Results:\")\n",
    "    dates = sorted(set(r['date'][:10] for r in results['results'] if r['date']))\n",
    "    for date in dates[:5]:\n",
    "        count = sum(1 for r in results['results'] if r['date'][:10] == date)\n",
    "        print(f\"   {date}: {count} relevant chunks\")\n",
    "        \n",
    "    print(f\"\\nüîç Top Result:\")\n",
    "    top = results['results'][0]\n",
    "    print(f\"   {top['filename']} ({top['date'][:10]})\")\n",
    "    print(f\"   {top['chunk_text'][:250]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-Based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Topic Analysis\n",
    "print(\"üìä Example 4: Topic-Based Search Results\\n\")\n",
    "\n",
    "topics = {\n",
    "    'Monetary Policy': 'monetary_policy',\n",
    "    'Banking Regulation': 'banking_regulation', \n",
    "    'International Finance': 'international_finance'\n",
    "}\n",
    "\n",
    "topic_results = {}\n",
    "for topic_name, topic_key in topics.items():\n",
    "    results = search.search_by_topic(topic_key, max_results=10)\n",
    "    topic_results[topic_name] = results['total_results']\n",
    "    \n",
    "    # Show year distribution for this topic\n",
    "    if results['results']:\n",
    "        years = [r['date'][:4] for r in results['results'] if r['date']]\n",
    "        year_dist = pd.Series(years).value_counts().sort_index()\n",
    "        \n",
    "        print(f\"üìà {topic_name}:\")\n",
    "        print(f\"   Total results: {results['total_results']}\")\n",
    "        print(f\"   Year distribution: {dict(year_dist.head(3))}\")\n",
    "        print()\n",
    "\n",
    "# Visualize topic distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(topic_results.keys(), topic_results.values(), color=['steelblue', 'darkgreen', 'darkred'])\n",
    "plt.title('Fed Minutes: Topic Search Results Distribution')\n",
    "plt.ylabel('Number of Relevant Chunks')\n",
    "plt.xticks(rotation=15)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### üéØ Knowledge Base Capabilities\n",
    "\n",
    "This Fed Minutes Knowledge Base provides:\n",
    "\n",
    "**‚úÖ Core Features:**\n",
    "- **Semantic search** across 1,100+ Fed meeting minutes\n",
    "- **Date-filtered searches** for specific time periods\n",
    "- **Topic-based analysis** with predefined categories\n",
    "- **Sub-second search performance** with rich metadata\n",
    "\n",
    "**üî¨ Research Applications:**\n",
    "- Historical policy analysis during critical periods\n",
    "- Decision-making pattern recognition\n",
    "- Institutional behavior studies\n",
    "- Economic event impact assessment\n",
    "- Cross-temporal policy comparison\n",
    "\n",
    "**üöÄ Next Phase:**\n",
    "Phase 3 will add AI-powered analysis including:\n",
    "- RAG (Retrieval-Augmented Generation) for intelligent Q&A\n",
    "- Automated insight generation and report creation\n",
    "- Advanced pattern recognition and anomaly detection\n",
    "- Natural language research query interface\n",
    "\n",
    "---\n",
    "*The knowledge base successfully transforms static historical documents into an intelligent research platform for understanding Federal Reserve decision-making during the pivotal 1965-1973 period.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}